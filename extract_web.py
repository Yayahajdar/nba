import os
import pandas as pd
import requests
from bs4 import BeautifulSoup
from io import StringIO

# -------------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¬Ù„Ø¯
# -------------------------------
RAW_DIR = "data/raw"
os.makedirs(RAW_DIR, exist_ok=True)

def run_web():
    url = "https://en.wikipedia.org/wiki/NBA_Most_Valuable_Player_Award"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://en.wikipedia.org/",
    }
    response = None
    for attempt in range(3):
        resp = requests.get(url, headers=headers, timeout=20)
        if resp.status_code == 200:
            response = resp
            break
        print(f"âš ï¸ Wikipedia fetch attempt {attempt+1}/3 failed with {resp.status_code}. Retrying...")
        time.sleep(2 ** attempt * 2)

    if response is None or response.status_code != 200:
        print("âš ï¸ Could not fetch Wikipedia page after retries. Skipping web extraction.")
        return []

    soup = BeautifulSoup(response.text, "html.parser")

    # Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
    tables = soup.find_all("table", {"class": "wikitable"})
    print(f"ğŸ” Found {len(tables)} wikitable tables on the page")

    # Ø­ÙØ¸ ÙƒÙ„ Ø¬Ø¯ÙˆÙ„
    for idx, table in enumerate(tables):
        # ØªØ­ÙˆÙŠÙ„ HTML Ø¥Ù„Ù‰ DataFrame
        df = pd.read_html(str(table))[0]

        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„ÙØ§Ø±ØºØ©
        df = df.dropna(how="all")
        df.to_csv(f"{RAW_DIR}/mvp_wiki_table_{idx+1}.csv", index=False)
        print(f"âœ… Table {idx+1} saved: {len(df)} rows, columns: {list(df.columns)}")
    
    print("ğŸ‰ All tables saved successfully!")
    return tables

if __name__ == "__main__":
    run_web()
